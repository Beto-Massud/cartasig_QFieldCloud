* QFieldCloud server
[[./docs/assets/images/logo.png]]
[[https://github.com/opengisch/qfieldcloud/workflows/Deploy%20on%20dev.qfield.cloud/badge.svg]]
[[https://github.com/opengisch/status.qfield.cloud/workflows/dev.qfield.cloud%20APIs%20status/badge.svg]]
[[https://github.com/opengisch/status.qfield.cloud/workflows/app.qfield.cloud%20APIs%20status/badge.svg]]
** Purpose
   QFieldCloud is a service designed to synchronize projects and data
   between QGIS (+ QFieldSync plugin) and QField.

   Initially it will allow to replace the use of the cable to copy
   projects in QField and later it will also allow to synchronize data
   and view and edit projects via web interface.

   System documentation is [[https://github.com/opengisch/qfieldcloud/blob/master/docs/system_documentation.org][here]]

   Documentation about how QFieldCloud file's storage works is [[https://github.com/opengisch/qfieldcloud/blob/master/docs/storage.org][here]]

   Permissions documentation is [[https://github.com/opengisch/qfieldcloud/blob/master/docs/permissions.org][here]]
** Development
*** Launch a local instance
    Copy the =.env.example= into =.env= file and configure it to your
    desire with a good editor
    #+begin_src sh
      cp .env.example .env
      emacs .env
    #+end_src
    For a local development instance you don't need to configure
    =supervisord.conf= configuration files.

    To build development images and run the containers:
    #+begin_src sh
      docker-compose up -d --build
    #+end_src

    It will read =docker-compose.yml= and =docker-compose.override.yml=
    and start a django built-in server at =http://localhost:8000=

    Run the django database migrations
    #+begin_src sh
      docker-compose exec app python manage.py migrate
    #+end_src

    To lauch the RQ (Redis Queue) workers that runs the QGIS containers jobs:
    #+begin_src sh
      cd orchestrator
      pipenv install
      pipenv run python worker.py
    #+end_src

    You can check if everything seems to work correctly using the
    =status= command:
    #+begin_src sh
      docker-compose exec app python manage.py status
    #+end_src
*** Tests
    To run all the unit and functional tests (on a throwaway test
    database and a throwaway test storage directory):
    #+begin_src sh
      docker-compose run app python manage.py test
    #+end_src

    To run only a test module (e.g. =test_permission.py=)
    #+begin_src sh
      docker-compose run app python manage.py test qfieldcloud.core.tests.test_permission
    #+end_src
** Deployment
*** Servers
    QFieldCloud is published on two servers:
    - https://dev.qfield.cloud/ This is a testing instance for new
      features.
    - https://app.qfield.cloud/ This is the production instance. At
      the moment the deploy is done manually.

    On the servers, we need only the =docker-compose.yml= and not the
    "override" one. There are no mounted folders. To apply changes,
    the docker image must be re-built.
*** Launch a server instance
    Copy the =.env.example= into =.env= file and configure it to your
    desire with a good editor
    #+begin_src sh
      cp .env.example .env
      emacs .env
    #+end_src

    Create the directory for qfieldcloud logs and supervisor socket file
    #+begin_src sh
      mkdir /var/local/qfieldcloud
    #+end_src

    Configure the =.env= file to your desire with a good editor
    #+begin_src sh
      cd conf
      cp supervisord.conf.example supervisord.conf
      emacs supervisord.conf
    #+end_src

    Run and build the docker containers (*not the
    =docker-compose.override.yml= file*)
    #+begin_src sh
      docker-compose -f docker-compose.yml up -d --build
    #+end_src

    Run the django database migrations
    #+begin_src sh
      docker-compose -f docker-compose.yml exec app python manage.py migrate
    #+end_src

    Prepare and run supervisord with the workers
    #+begin_src sh
      apt install python3-pip  # If needed
      pip3 install pipenv  # If needed
      # You should upgrade your python version on your host to 3.8.3 or superior
      cd orchestrator
      pipenv --python /usr/bin/python3.8 install
      pipenv run supervisord -c ../conf/supervisord.conf
    #+end_src

    You can watch and control the processes managed with supervisor
    with ~supervisorctl~.

    #+begin_src sh
      pipenv run supervisorctl -c ../conf/supervisord.conf
    #+end_src
    For details see the offical [[http://supervisord.org/running.html#running-supervisorctl][documentation]]
*** Infrastructure
    Based on this example
    https://testdriven.io/blog/dockerizing-django-with-postgres-gunicorn-and-nginx/
*** Logs
    Docker logs are managed by docker in the default way. To read the logs:
    #+begin_src sh
      docker-compose -f docker-compose.yml logs
    #+end_src

    Orchestrator logs are stored in the ~/var/local/qfieldcloud/~ directory
*** Geodb
    The geodb (database for the users projects data) is installed on
    separated machines (db1.qfield.cloud, db2.qfield.cloud, db3...)
    and they are load balanced and available through the
    db.qfield.cloud address.

    There is a template database called
    =template_postgis= that is used to create the databases for the
    users. The template db has the following extensions installed:
    - fuzzystrmatch
    - plpgsql
    - postgis
    - postgis_tiger_geocoder
    - postgis_topology
** Resources
   - [[https://qfield.cloud][QField Cloud "marketing" page]]
   - [[https://app.qfield.cloud/swagger/][API Swagger doc]]
   - [[http://status.qfield.cloud/][API status page]]

